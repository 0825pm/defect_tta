"""
Severstal Steel Defect Detection - Dataset Preparation
======================================================
ë‹¨ì¼ ë¶ˆëŸ‰ ì´ë¯¸ì§€ë¥¼ train/valë¡œ, ë‹¤ì¤‘ ë¶ˆëŸ‰ ì´ë¯¸ì§€ë¥¼ testë¡œ ë¶„í• 

Usage:
    python prepare_dataset.py --data_root /path/to/severstal --output_dir ./splits
"""

import os
import argparse
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from collections import Counter
import json


def parse_args():
    parser = argparse.ArgumentParser(description='Prepare Severstal dataset splits')
    parser.add_argument('--data_root', type=str, required=True,
                        help='Path to Severstal dataset root')
    parser.add_argument('--output_dir', type=str, default='./splits',
                        help='Output directory for split CSVs')
    parser.add_argument('--val_ratio', type=float, default=0.2,
                        help='Validation ratio from single-defect images')
    parser.add_argument('--seed', type=int, default=42,
                        help='Random seed for reproducibility')
    return parser.parse_args()


def load_and_analyze_data(data_root):
    """
    train.csv ë¡œë“œ ë° ë¶„ì„
    
    Returns:
        df: ì›ë³¸ DataFrame
        image_info: ì´ë¯¸ì§€ë³„ ì •ë³´ (ë¶ˆëŸ‰ ê°œìˆ˜, í´ë˜ìŠ¤ ëª©ë¡)
    """
    train_csv_path = os.path.join(data_root, 'train.csv')
    
    if not os.path.exists(train_csv_path):
        raise FileNotFoundError(f"train.csv not found at {train_csv_path}")
    
    df = pd.read_csv(train_csv_path)
    print(f"âœ“ Loaded train.csv: {len(df)} rows")
    
    # ê²°ì¸¡ê°’ ì²˜ë¦¬ (EncodedPixelsê°€ NaNì¸ ê²½ìš° = ë¶ˆëŸ‰ ì—†ìŒ)
    df = df.dropna(subset=['EncodedPixels'])
    print(f"âœ“ After removing NaN: {len(df)} rows")
    
    # ì´ë¯¸ì§€ë³„ ì •ë³´ ì§‘ê³„
    image_info = df.groupby('ImageId').agg({
        'ClassId': list,
        'EncodedPixels': list
    }).reset_index()
    
    image_info['num_defects'] = image_info['ClassId'].apply(len)
    image_info['defect_classes'] = image_info['ClassId'].apply(lambda x: sorted(set(x)))
    
    # ë‹¨ì¼ ë¶ˆëŸ‰ì˜ ê²½ìš° í´ë˜ìŠ¤ ë ˆì´ë¸” (stratified splitìš©)
    image_info['primary_class'] = image_info['ClassId'].apply(
        lambda x: x[0] if len(x) == 1 else -1  # ë‹¤ì¤‘ ë¶ˆëŸ‰ì€ -1
    )
    
    print(f"âœ“ Total unique images: {len(image_info)}")
    
    return df, image_info


def analyze_distribution(image_info):
    """ë°ì´í„° ë¶„í¬ ë¶„ì„ ë° ì¶œë ¥"""
    print("\n" + "="*60)
    print("ğŸ“Š ë°ì´í„° ë¶„í¬ ë¶„ì„")
    print("="*60)
    
    # ë¶ˆëŸ‰ ê°œìˆ˜ë³„ ë¶„í¬
    defect_counts = image_info['num_defects'].value_counts().sort_index()
    print("\në¶ˆëŸ‰ ê°œìˆ˜ë³„ ì´ë¯¸ì§€ ìˆ˜:")
    for num, count in defect_counts.items():
        pct = count / len(image_info) * 100
        print(f"  {num}ê°œ ë¶ˆëŸ‰: {count:,} ({pct:.1f}%)")
    
    # ë‹¨ì¼ ë¶ˆëŸ‰ í´ë˜ìŠ¤ ë¶„í¬
    single_defect = image_info[image_info['num_defects'] == 1]
    class_dist = single_defect['primary_class'].value_counts().sort_index()
    print("\në‹¨ì¼ ë¶ˆëŸ‰ í´ë˜ìŠ¤ ë¶„í¬:")
    for cls, count in class_dist.items():
        pct = count / len(single_defect) * 100
        print(f"  Class {cls}: {count:,} ({pct:.1f}%)")
    
    # ë‹¤ì¤‘ ë¶ˆëŸ‰ ì¡°í•© ë¶„í¬
    multi_defect = image_info[image_info['num_defects'] > 1]
    if len(multi_defect) > 0:
        combo_dist = multi_defect['defect_classes'].apply(tuple).value_counts()
        print("\në‹¤ì¤‘ ë¶ˆëŸ‰ ì¡°í•© ë¶„í¬ (ìƒìœ„ 10ê°œ):")
        for combo, count in combo_dist.head(10).items():
            print(f"  {list(combo)}: {count}")
    
    return {
        'total_images': len(image_info),
        'single_defect': len(single_defect),
        'multi_defect': len(multi_defect),
        'class_distribution': class_dist.to_dict()
    }


def split_dataset(image_info, val_ratio=0.2, seed=42):
    """
    ë°ì´í„°ì…‹ ë¶„í• 
    - ë‹¨ì¼ ë¶ˆëŸ‰ â†’ train/val (stratified by class)
    - ë‹¤ì¤‘ ë¶ˆëŸ‰ â†’ test
    """
    print("\n" + "="*60)
    print("ğŸ”€ ë°ì´í„°ì…‹ ë¶„í• ")
    print("="*60)
    
    # ë‹¨ì¼ ë¶ˆëŸ‰ vs ë‹¤ì¤‘ ë¶ˆëŸ‰ ë¶„ë¦¬
    single_defect = image_info[image_info['num_defects'] == 1].copy()
    multi_defect = image_info[image_info['num_defects'] > 1].copy()
    
    print(f"\në‹¨ì¼ ë¶ˆëŸ‰ ì´ë¯¸ì§€: {len(single_defect):,}")
    print(f"ë‹¤ì¤‘ ë¶ˆëŸ‰ ì´ë¯¸ì§€: {len(multi_defect):,}")
    
    # ë‹¨ì¼ ë¶ˆëŸ‰ì„ train/valë¡œ stratified split
    train_images, val_images = train_test_split(
        single_defect,
        test_size=val_ratio,
        stratify=single_defect['primary_class'],
        random_state=seed
    )
    
    # Split íƒœê·¸ ì¶”ê°€
    train_images = train_images.copy()
    val_images = val_images.copy()
    multi_defect = multi_defect.copy()
    
    train_images['split'] = 'train'
    val_images['split'] = 'val'
    multi_defect['split'] = 'test'
    
    print(f"\në¶„í•  ê²°ê³¼:")
    print(f"  Train: {len(train_images):,} ({len(train_images)/len(single_defect)*100:.1f}%)")
    print(f"  Val:   {len(val_images):,} ({len(val_images)/len(single_defect)*100:.1f}%)")
    print(f"  Test:  {len(multi_defect):,} (ë‹¤ì¤‘ ë¶ˆëŸ‰ ì „ì²´)")
    
    # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
    print("\ní´ë˜ìŠ¤ ë¶„í¬ ê²€ì¦:")
    for split_name, split_df in [('Train', train_images), ('Val', val_images)]:
        dist = split_df['primary_class'].value_counts().sort_index()
        dist_str = ', '.join([f"C{c}:{n}" for c, n in dist.items()])
        print(f"  {split_name}: {dist_str}")
    
    return train_images, val_images, multi_defect


def create_split_csv(df_original, train_info, val_info, test_info, output_dir):
    """
    ë¶„í•  ì •ë³´ë¥¼ CSVë¡œ ì €ì¥
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. Split info CSV (ê°„ë‹¨ ë²„ì „)
    all_info = pd.concat([train_info, val_info, test_info], ignore_index=True)
    split_info = all_info[['ImageId', 'split', 'num_defects', 'primary_class']].copy()
    split_info['defect_classes'] = all_info['defect_classes'].apply(
        lambda x: ','.join(map(str, x))
    )
    
    split_info_path = os.path.join(output_dir, 'split_info.csv')
    split_info.to_csv(split_info_path, index=False)
    print(f"\nâœ“ Saved: {split_info_path}")
    
    # 2-4. ê° splitë³„ ìƒì„¸ CSV (ì›ë³¸ í˜•ì‹ ìœ ì§€)
    for split_name, info_df in [('train', train_info), ('val', val_info), ('test', test_info)]:
        image_ids = set(info_df['ImageId'].tolist())
        split_df = df_original[df_original['ImageId'].isin(image_ids)].copy()
        split_df['split'] = split_name
        
        split_path = os.path.join(output_dir, f'{split_name}.csv')
        split_df.to_csv(split_path, index=False)
        print(f"âœ“ Saved: {split_path} ({len(split_df)} rows, {len(image_ids)} images)")
    
    return split_info_path


def save_statistics(stats, train_info, val_info, test_info, output_dir):
    """í†µê³„ ì •ë³´ JSONìœ¼ë¡œ ì €ì¥"""
    stats['splits'] = {
        'train': {
            'num_images': len(train_info),
            'class_distribution': train_info['primary_class'].value_counts().sort_index().to_dict()
        },
        'val': {
            'num_images': len(val_info),
            'class_distribution': val_info['primary_class'].value_counts().sort_index().to_dict()
        },
        'test': {
            'num_images': len(test_info),
            'num_defects_distribution': test_info['num_defects'].value_counts().sort_index().to_dict()
        }
    }
    
    stats_path = os.path.join(output_dir, 'dataset_stats.json')
    with open(stats_path, 'w') as f:
        json.dump(stats, f, indent=2)
    print(f"âœ“ Saved: {stats_path}")


def main():
    args = parse_args()
    
    print("="*60)
    print("ğŸ”§ Severstal Dataset Preparation")
    print("="*60)
    print(f"Data root: {args.data_root}")
    print(f"Output dir: {args.output_dir}")
    print(f"Val ratio: {args.val_ratio}")
    print(f"Seed: {args.seed}")
    
    # 1. ë°ì´í„° ë¡œë“œ
    df, image_info = load_and_analyze_data(args.data_root)
    
    # 2. ë¶„í¬ ë¶„ì„
    stats = analyze_distribution(image_info)
    
    # 3. ë°ì´í„° ë¶„í• 
    train_info, val_info, test_info = split_dataset(
        image_info, 
        val_ratio=args.val_ratio, 
        seed=args.seed
    )
    
    # 4. CSV ì €ì¥
    create_split_csv(df, train_info, val_info, test_info, args.output_dir)
    
    # 5. í†µê³„ ì €ì¥
    save_statistics(stats, train_info, val_info, test_info, args.output_dir)
    
    print("\n" + "="*60)
    print("âœ… Dataset preparation complete!")
    print("="*60)


if __name__ == '__main__':
    main()